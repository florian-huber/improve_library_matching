{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 525,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import gensim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "ROOT = os.path.dirname(os.getcwd())\n",
    "#path_data = os.path.join(ROOT, 'data')\n",
    "path_data = 'C:\\\\Users\\\\joris\\\\Documents\\\\eScience_data\\\\data'\n",
    "sys.path.insert(0, ROOT)\n",
    "sys.path.insert(0, \"C:\\\\Users\\\\joris\\\\Documents\\\\eScience_data\\\\spec2vec_gnps_data_analysis\\\\custom_functions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import pre-processed dataset \"AllPositive\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pickle\n",
    "\n",
    "#loading data as pickled object goes a lot quicker\n",
    "# outfile = os.path.join(path_data, 'gnps_positive_ionmode_cleaned_by_matchms_and_lookups.pickle')\n",
    "\n",
    "# print(outfile)\n",
    "# start = time.time()\n",
    "# with open(outfile, 'rb') as inf:\n",
    "#         spectrums = pickle.load(inf)\n",
    "# end = time.time()\n",
    "# print('loading took {:.2f} s'.format(end-start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import spec2vec matches of 1,000 spectra where their inchikeys occur >=5 times in the library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\joris\\Documents\\eScience_data\\data\\found_matches_s2v.pickle\n",
      "C:\\Users\\joris\\Documents\\eScience_data\\data\\documents_query_s2v.pickle\n",
      "C:\\Users\\joris\\Documents\\eScience_data\\data\\documents_library_s2v.pickle\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-527-339523b63464>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0minf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m         \u001b[0mdocuments_library_s2v\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "outfile = os.path.join(path_data, 'found_matches_s2v.pickle')\n",
    "print(outfile)\n",
    "if os.path.exists(outfile):\n",
    "    with open(outfile, 'rb') as inf:\n",
    "        found_matches_s2v = pickle.load(inf)\n",
    "else:\n",
    "    print('error')\n",
    "\n",
    "#contains info on the query/library\n",
    "outfile = os.path.join(path_data, 'documents_query_s2v.pickle')\n",
    "print(outfile)\n",
    "if os.path.exists(outfile):\n",
    "    with open(outfile, 'rb') as inf:\n",
    "        documents_query_s2v = pickle.load(inf)\n",
    "\n",
    "outfile = os.path.join(path_data, 'documents_library_s2v.pickle')\n",
    "print(outfile)\n",
    "if os.path.exists(outfile):\n",
    "    with open(outfile, 'rb') as inf:\n",
    "        documents_library_s2v = pickle.load(inf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#and another set of 1,000 different spectra\n",
    "#contains info on the query/library\n",
    "outfile = os.path.join(path_data, 'new_documents_query_s2v.pickle')\n",
    "print(outfile)\n",
    "if os.path.exists(outfile):\n",
    "    with open(outfile, 'rb') as inf:\n",
    "        new_documents_query_s2v = pickle.load(inf)\n",
    "\n",
    "outfile = os.path.join(path_data, 'new_documents_library_s2v.pickle')\n",
    "print(outfile)\n",
    "if os.path.exists(outfile):\n",
    "    with open(outfile, 'rb') as inf:\n",
    "        new_documents_library_s2v = pickle.load(inf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate pairwise tanimoto similarity between top 20 results for each query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from rdkit import Chem, DataStructs\n",
    "\n",
    "outfile = os.path.join(path_data, 'similarity_matrices_test_set1.pickle')\n",
    "print(outfile)\n",
    "if os.path.exists(outfile):\n",
    "    with open(outfile, 'rb') as inf:\n",
    "        similarity_matrices = pickle.load(inf)\n",
    "else:\n",
    "    #for each matches df make a similarity matrix based on tanimoto\n",
    "    similarity_matrices = []\n",
    "    topn = 20\n",
    "\n",
    "    for ID in range(len(found_matches_s2v)):\n",
    "        df = found_matches_s2v[ID].sort_values(by=['s2v_score'], ascending = False).iloc[:topn]\n",
    "        library_ids = df.index.values\n",
    "        rows = []\n",
    "        for lib_id_i in library_ids:\n",
    "            smiles_i = documents_library_s2v[lib_id_i]._obj.get(\"smiles\")\n",
    "\n",
    "            if not smiles_i or smiles_i == \"None\": # check that lib_id_i smiles exist\n",
    "                rows.append([0] * len(library_ids)) #default to all 0 if it doesnt exist\n",
    "                continue\n",
    "            ms_i = Chem.MolFromSmiles(smiles_i)\n",
    "            if not ms_i: #in case something is wrong with smiles\n",
    "                rows.append([0] * len(library_ids)) #default to all 0 if it doesnt exist\n",
    "                continue\n",
    "            fp_i = Chem.RDKFingerprint(ms_i)\n",
    "\n",
    "            row = []\n",
    "            for lib_id_j in library_ids:\n",
    "                smiles_j = documents_library_s2v[lib_id_j]._obj.get(\"smiles\")\n",
    "\n",
    "                if smiles_j and smiles_j != \"None\":\n",
    "                    ms_j = Chem.MolFromSmiles(smiles_j)\n",
    "                    if ms_j:\n",
    "                        fp_j = Chem.RDKFingerprint(ms_j)\n",
    "                        score = DataStructs.FingerprintSimilarity(fp_i, fp_j)\n",
    "                    else: #in case something is wrong with smiles\n",
    "                        score = 0\n",
    "                else: #in case it doesnt have smiles\n",
    "                    score = 0\n",
    "                row.append(score)\n",
    "\n",
    "            rows.append(row)\n",
    "        similarity_matrices.append(pd.DataFrame(rows))\n",
    "    #write to file    \n",
    "    with open(outfile, 'wb') as outf:\n",
    "        pickle.dump(similarity_matrices, outf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#corrected version where true matches get a slightly higher value in the diagonal\n",
    "import pickle\n",
    "cor_val = 1.001\n",
    "outfile = os.path.join(path_data, 'similarity_matrices_corrected_test_set1.pickle')\n",
    "print(outfile)\n",
    "if os.path.exists(outfile):\n",
    "    with open(outfile, 'rb') as inf:\n",
    "        similarity_matrices_cor = pickle.load(inf)\n",
    "else:\n",
    "    #for each matches df make a similarity matrix based on tanimoto\n",
    "    similarity_matrices_cor = []\n",
    "    topn = 20\n",
    "\n",
    "    for ID in range(len(found_matches_s2v)):\n",
    "        df = found_matches_s2v[ID].sort_values(by=['s2v_score'], ascending = False).iloc[:topn]\n",
    "        library_ids = df.index.values\n",
    "        rows = []\n",
    "        for lib_id_i in library_ids:\n",
    "            smiles_i = documents_library_s2v[lib_id_i]._obj.get(\"smiles\")\n",
    "\n",
    "            if not smiles_i or smiles_i == \"None\": # check that lib_id_i smiles exist\n",
    "                rows.append([0] * len(library_ids)) #default to all 0 if it doesnt exist\n",
    "                continue\n",
    "            ms_i = Chem.MolFromSmiles(smiles_i)\n",
    "            if not ms_i: #in case something is wrong with smiles\n",
    "                rows.append([0] * len(library_ids)) #default to all 0 if it doesnt exist\n",
    "                continue\n",
    "            fp_i = Chem.RDKFingerprint(ms_i)\n",
    "\n",
    "            row = []\n",
    "            for lib_id_j in library_ids:\n",
    "                if lib_id_i == lib_id_j: #if diagonal see if its a true match and give it a slightly higher value for plotting\n",
    "                    df_select = found_matches_s2v_match_perc_pmass_sim[ID].sort_values(by=['s2v_score'], ascending = False).iloc[:topn]\n",
    "                    if df_select.loc[lib_id_j]['label'] == 1:\n",
    "                        score = cor_val\n",
    "                    else:\n",
    "                        score = 1\n",
    "                    \n",
    "                else:\n",
    "                    smiles_j = documents_library_s2v[lib_id_j]._obj.get(\"smiles\")\n",
    "\n",
    "                    if smiles_j and smiles_j != \"None\":\n",
    "                        ms_j = Chem.MolFromSmiles(smiles_j)\n",
    "                        if ms_j:\n",
    "                            fp_j = Chem.RDKFingerprint(ms_j)\n",
    "                            score = DataStructs.FingerprintSimilarity(fp_i, fp_j)\n",
    "                        else: #in case something is wrong with smiles\n",
    "                            score = 0\n",
    "                    else: #in case it doesnt have smiles\n",
    "                        score = 0\n",
    "                row.append(score)\n",
    "\n",
    "            rows.append(row)\n",
    "        similarity_matrices_cor.append(pd.DataFrame(rows))\n",
    "    #write to file\n",
    "    with open(outfile, 'wb') as outf:\n",
    "        pickle.dump(similarity_matrices_cor, outf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_select = found_matches_s2v_match_perc_pmass_sim[999].sort_values(by=['s2v_score'], ascending = False).iloc[:20]\n",
    "true_positives = [i for i, lab in enumerate(df_select['label']) if lab == 1]\n",
    "df_select.loc[lib_id_j]['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rows = similarity_matrices_cor[9]\n",
    "df_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import cm\n",
    "from matplotlib.colors import ListedColormap, LinearSegmentedColormap\n",
    "\n",
    "x = 100000\n",
    "inferno = cm.get_cmap('inferno', x)\n",
    "green = np.array([50/256, 205/256, 50/256, 1])\n",
    "new_colours = np.vstack((inferno(np.linspace(0, 1, x)), green))\n",
    "new_cmap = ListedColormap(new_colours, name='inferno-green')\n",
    "plt.imshow(df_rows, cmap=new_cmap, vmin=0, vmax=cor_val)\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(4, 5, figsize=(14,10))\n",
    "for i, ax in enumerate(axs.flat):\n",
    "    im = ax.imshow(similarity_matrices[i], cmap='inferno', vmin = 0, vmax = 1)\n",
    "\n",
    "fig.subplots_adjust(right=0.95)\n",
    "cbar_ax = fig.add_axes([0.97, 0.15, 0.02, 0.69])\n",
    "fig.colorbar(im, cax=cbar_ax)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(4, 5, figsize=(14,10))\n",
    "for i, ax in enumerate(axs.flat):\n",
    "    im = ax.imshow(similarity_matrices_cor[i], cmap=new_cmap, vmax=cor_val, vmin=0)\n",
    "\n",
    "fig.subplots_adjust(right=0.95)\n",
    "cbar_ax = fig.add_axes([0.97, 0.15, 0.02, 0.69])\n",
    "fig.colorbar(im, cax=cbar_ax)\n",
    "\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#same plot but only the ones where the top hit is not the true hit\n",
    "plt.style.use('default')\n",
    "fig, axs = plt.subplots(4, 5, figsize=(13,9))\n",
    "for i, ax in enumerate(axs.flat):\n",
    "    im = ax.imshow(similarity_matrices_cor[ids_top_false_hits_thresh[i]], cmap=new_cmap, vmax=cor_val, vmin=0)\n",
    "\n",
    "fig.subplots_adjust(right=0.95)\n",
    "cbar_ax = fig.add_axes([0.97, 0.15, 0.02, 0.69])\n",
    "fig.colorbar(im, cax=cbar_ax)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outfile = os.path.join(path_data, 'rfreg_all_match_perc_pmass.pickle')\n",
    "print(outfile)\n",
    "if os.path.exists(outfile):\n",
    "    with open(outfile, 'rb') as inf:\n",
    "        rfreg_all_match_perc_pmass = pickle.load(inf)\n",
    "\n",
    "outfile = os.path.join(path_data, 'found_matches_s2v_match_perc_pmass_sim.pickle')\n",
    "print(outfile)\n",
    "if os.path.exists(outfile):\n",
    "    with open(outfile, 'rb') as inf:\n",
    "        found_matches_s2v_match_perc_pmass_sim = pickle.load(inf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "found_matches_s2v_match_perc_pmass_sim[8].sort_values(by=['s2v_score'], ascending = False).iloc[:40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualise tanimoto score of query vs match per position\n",
    "topn = 20\n",
    "all_similarities = []\n",
    "for ID in range(len(found_matches_s2v_match_perc_pmass_sim)):\n",
    "    df_select = found_matches_s2v_match_perc_pmass_sim[ID].sort_values(by=['s2v_score'], ascending = False).iloc[:topn]\n",
    "    simil = df_select['similarity']\n",
    "    all_similarities.append(list(simil))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "per_pos = list(zip(*all_similarities))\n",
    "#print(per_pos)\n",
    "means = list(map(np.mean, per_pos))\n",
    "stdevs = list(map(np.std, per_pos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(range(len(means)), means, yerr = stdevs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for each best true hit, how many connections > 0.6 does it have?\n",
    "topn = 20\n",
    "ID=0\n",
    "cutoff = 0.6\n",
    "amount_sims = []\n",
    "for ID in range(len(found_matches_s2v_match_perc_pmass_sim)):\n",
    "    df_select = found_matches_s2v_match_perc_pmass_sim[ID].sort_values(by=['s2v_score'], ascending = False).iloc[:topn]\n",
    "    true_positives = [i for i, lab in enumerate(df_select['label']) if lab == 1]\n",
    "    try:\n",
    "        true_pos = true_positives[0] #take top hit\n",
    "    except IndexError:\n",
    "        amount_sim = -1 #true positive can't be found in topn\n",
    "    else:\n",
    "        amount_sim = len(similarity_matrices[ID][similarity_matrices[ID][true_pos] > cutoff])\n",
    "    #report max cases\n",
    "    #if amount_sim == 20:\n",
    "        #print(ID)\n",
    "    amount_sims.append(amount_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(amount_sims, bins = range(-1, max(amount_sims)+1))\n",
    "plt.xlabel('Connections above 0.6')\n",
    "plt.title('Amount of connections above 0.6 for each best true match')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if top hit is not true match, what is the average similarity of top hit above a threshold?\n",
    "topn = 20\n",
    "threshold = 0.4\n",
    "false_m_simils_thresh = []\n",
    "ids_top_false_hits_thresh = []\n",
    "for ID in range(len(found_matches_s2v_match_perc_pmass_sim)):\n",
    "    df_select = found_matches_s2v_match_perc_pmass_sim[ID].sort_values(by=['s2v_score'], ascending = False).iloc[:topn]\n",
    "    df_select = df_select[(df_select['s2v_score'] > threshold)]\n",
    "    if df_select.shape[0] > 0:\n",
    "        top_hit = df_select.iloc[0]\n",
    "        if top_hit['label'] == 0:\n",
    "            false_m_simil = top_hit['similarity']\n",
    "            print(ID,false_m_simil)\n",
    "            ids_top_false_hits_thresh.append(ID)\n",
    "            false_m_simils_thresh.append(false_m_simil)\n",
    "print(len(false_m_simils_thresh))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(2,2))\n",
    "plt.boxplot(false_m_simils_thresh)\n",
    "plt.show()\n",
    "plt.close()\n",
    "pd.DataFrame(false_m_simils_thresh, columns = ['avg_similarity_false_top_hit']).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "found_matches_s2v_info_connections = []\n",
    "cutoff = 0.6\n",
    "connections = [] #list of lists, just the connections column for each query\n",
    "for ID in range(len(found_matches_s2v_match_perc_pmass_sim)):\n",
    "    df_select = found_matches_s2v_match_perc_pmass_sim[ID].sort_values(by=['s2v_score'], ascending = False).iloc[:topn]\n",
    "    sim_matrix_select = similarity_matrices[ID]\n",
    "    amount_sims = []\n",
    "    for inner_ID in range(df_select.shape[0]):\n",
    "        amount_sim = len(sim_matrix_select[sim_matrix_select[inner_ID] > cutoff])\n",
    "        amount_sims.append(amount_sim)\n",
    "    new_df = df_select.copy()\n",
    "    new_df['connections'] = amount_sims\n",
    "    connections.append(amount_sims)\n",
    "    found_matches_s2v_info_connections.append(new_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "found_matches_s2v_info_connections[13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make new score which is s2v_score * factor^connections\n",
    "factor = 1.04\n",
    "cor_scores = []\n",
    "found_matches_s2v_info_new_score = []\n",
    "for ID in range(len(found_matches_s2v_info_connections)):\n",
    "    cur_connections = connections[ID]\n",
    "    cur_df = found_matches_s2v_info_connections[ID].copy()\n",
    "    s2v_scores = cur_df['s2v_score']\n",
    "    cor_score = []\n",
    "    for con, s2v_sc in zip(cur_connections, s2v_scores):\n",
    "        new_score = s2v_sc * (factor ** con)\n",
    "        cor_score.append(new_score)\n",
    "    cor_scores.append(cor_score)\n",
    "    cur_df['new_score'] = cor_score\n",
    "    found_matches_s2v_info_new_score.append(cur_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "found_matches_s2v_info_new_score[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing new_score + mass\n",
    "#look at top20\n",
    "test_m_s2v_top20 = []\n",
    "test_m_new_score_top20 = []\n",
    "threshold = 0.4\n",
    "topn = 20 #only look at top 20\n",
    "\n",
    "for ID in range(len(documents_query_s2v)):\n",
    "    inchikey_query = documents_query_s2v[ID]._obj.get(\"inchikey\")[:14]\n",
    "    #select top20\n",
    "    current_match = found_matches_s2v[ID].sort_values(by=['s2v_score'], ascending=False).iloc[:topn]\n",
    "\n",
    "    # Scenario 2: mass + Spec2Vec + threshold\n",
    "    df_select = current_match[(current_match['mass_match'] == 1)\n",
    "                                  & (current_match['s2v_score'] > threshold)]\n",
    "    if df_select.shape[0] > 0:\n",
    "        best_match_ID = df_select.sort_values(by=['s2v_score'], ascending=False).index[0]\n",
    "        inchikey_match = documents_library_s2v[best_match_ID]._obj.get(\"inchikey\")[:14]\n",
    "        best_bet = 1 * (inchikey_match == inchikey_query)\n",
    "    else:\n",
    "        best_bet = -1 # meaning: not found\n",
    "    test_m_s2v_top20.append(best_bet)\n",
    "    \n",
    "    # Scenario 2: mass + new_score + threshold\n",
    "    current_match_new = found_matches_s2v_info_new_score[ID].sort_values(by=['new_score'], ascending=False).iloc[:topn]\n",
    "    df_select = current_match_new[(current_match_new['mass_match'] == 1)\n",
    "                                  & (current_match_new['new_score'] > threshold)]\n",
    "    if df_select.shape[0] > 0:\n",
    "        best_match_ID = df_select.sort_values(by=['s2v_score'], ascending=False).index[0]\n",
    "        inchikey_match = documents_library_s2v[best_match_ID]._obj.get(\"inchikey\")[:14]\n",
    "        best_bet = 1 * (inchikey_match == inchikey_query)\n",
    "    else:\n",
    "        best_bet = -1 # meaning: not found\n",
    "    test_m_new_score_top20.append(best_bet)\n",
    "\n",
    "\n",
    "#make arrays from lists\n",
    "test_m_s2v_arr_top20 = np.array(test_m_s2v_top20)\n",
    "test_m_new_score_arr_top20 = np.array(test_m_new_score_top20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('ggplot')\n",
    "\n",
    "ylabels = ['mass + Spec2Vec',\n",
    "           'mass + new_score',]\n",
    "xlabels = ['true matches', 'false matches', 'no matches found']\n",
    "          #'mass + mod.cosine \\n + as backup: Spec2Vec + mod.cosine']\n",
    "data = np.array([[np.sum(test_m_s2v_arr_top20 == 1), np.sum(test_m_s2v_arr_top20 == 0), np.sum(test_m_s2v_arr_top20 == -1)],\n",
    "                 [np.sum(test_m_new_score_arr_top20 == 1), np.sum(test_m_new_score_arr_top20 == 0), np.sum(test_m_new_score_arr_top20 == -1)]])\n",
    "\n",
    "dataframe = pd.DataFrame(data, columns=xlabels, index=ylabels)\n",
    "\n",
    "ax = dataframe.sort_values(by=['false matches'], ascending=True).plot.barh(stacked=True, edgecolor='none')\n",
    "\n",
    "horiz_offset = 1.03\n",
    "vert_offset = 1.\n",
    "ax.legend(bbox_to_anchor=(horiz_offset, vert_offset))\n",
    "plt.xlabel('Number of spectra')\n",
    "plt.title('Comparing new_score vs spec2vec looking at top 20\\n(>5 matching inchikey in library)')\n",
    "#plt.figure(figsize=(5,5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prep data for NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualise mod_cosine_matches for all top20 results\n",
    "mod_cosine_matches = [elem for ID in range(len(found_matches_s2v_info_new_score)) for elem in list(found_matches_s2v_info_new_score[ID]['mod_cosine_matches'])]\n",
    "plt.hist(mod_cosine_matches, bins = max(mod_cosine_matches)+1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualise cosine_matches for all top20 results\n",
    "cosine_matches = [elem for ID in range(len(found_matches_s2v_info_new_score)) for elem in list(found_matches_s2v_info_new_score[ID]['cosine_matches'])]\n",
    "plt.hist(cosine_matches, bins = max(cosine_matches)+1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing scaling\n",
    "x = 15\n",
    "base = 0.8\n",
    "dist = [(base**i) for i in range(x)]\n",
    "plt.scatter(range(x), dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_num_matches(input_df, exp = 0.93):\n",
    "    '''Transform the cosine_matches and mod_cosine_matches to between 0-1\n",
    "    \n",
    "    input_df: pandas DataFrame, spec2vec matches for one query\n",
    "    exp: int, the base for the exponential, default: 0.93\n",
    "    \n",
    "    Both matches are transformed to between 0-1 by doing 1-0.93^num_matches\n",
    "    '''\n",
    "    df = input_df.copy() #otherwise it edits the df outside the function\n",
    "    df['cosine_matches'] = [(1-0.93**i) for i in df['cosine_matches']]\n",
    "    df['mod_cosine_matches'] = [(1-0.93**i) for i in df['mod_cosine_matches']]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_mass_difference(matches, documents_library, query_mass, base_num = 0.8):\n",
    "    '''\n",
    "    To each match in matches df, add a scaled value for how many daltons it differs from query\n",
    "\n",
    "    matches: pandas DataFrame, library matching result of 1 query on library\n",
    "    documents_library: list of SpectrumDocuments, spectra in library\n",
    "    query_mass: float, parent mass of query\n",
    "    base_num: float, the base for the exponent\n",
    "    df: pandas DataFrame, library matching result of 1 query on library with mass diffs\n",
    "    \n",
    "    The difference in dalton is calculated and transformed into a value 0 - 1 by doing\n",
    "    base_num^diff_in_dalton\n",
    "    '''\n",
    "    df = matches.copy()\n",
    "    library_ids = df.index.values\n",
    "    scaled_mass_diffs = []\n",
    "    for lib_id in library_ids:\n",
    "        lib_mass = documents_library[lib_id]._obj.get(\"parent_mass\")\n",
    "        mass_diff = abs(lib_mass - query_mass)\n",
    "        scaled_mass_diff = base_num ** mass_diff\n",
    "        scaled_mass_diffs.append(scaled_mass_diff)\n",
    "\n",
    "    #add to df\n",
    "    df['mass_diff'] = scaled_mass_diffs\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get max parent mass to make it relative\n",
    "all_parent_masses = [elem for ID in range(len(found_matches_s2v_match_perc_pmass_sim)) for elem in list(found_matches_s2v_match_perc_pmass_sim[ID]['parent_mass'])]\n",
    "max_parent_mass = max(all_parent_masses)\n",
    "max_parent_mass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocess for neural network, remove mass_match and perc_mass_change, make number of matches relative\n",
    "#add new transform of mass difference\n",
    "nn_prep_found_matches_s2v_match_perc_pmass_sim = []\n",
    "for ID in range(len(found_matches_s2v_match_perc_pmass_sim)):\n",
    "    current = found_matches_s2v_match_perc_pmass_sim[ID].copy()\n",
    "    current = transform_num_matches(current)\n",
    "    current['parent_mass'] = [cur_pm/max_parent_mass for cur_pm in current['parent_mass']]\n",
    "    current.drop(['mass_match', 'perc_mass_change'], axis=1, inplace=True)\n",
    "    q_mass = documents_query_s2v[ID]._obj.get(\"parent_mass\")\n",
    "    current = find_mass_difference(current, documents_library_s2v, q_mass, base_num = 0.8)\n",
    "    nn_prep_found_matches_s2v_match_perc_pmass_sim.append(current)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#finalising data for nn, add label 1 if similarity is above cutoff\n",
    "sim_cutoff = 0.6\n",
    "nn_found_matches_s2v_match_perc_pmass_sim = nn_prep_found_matches_s2v_match_perc_pmass_sim[0]\\\n",
    "    .append(nn_prep_found_matches_s2v_match_perc_pmass_sim[1:])\\\n",
    "    .drop(['label'], axis = 1)\n",
    "simlabs = [1 if sim >= sim_cutoff else 0 for sim in nn_found_matches_s2v_match_perc_pmass_sim['similarity']]\n",
    "nn_found_matches_s2v_match_perc_pmass_sim['sim_label'] = simlabs\n",
    "nn_found_matches_s2v_match_perc_pmass_sim.drop(['similarity'], axis = 1, inplace = True)\n",
    "nn_found_matches_s2v_match_perc_pmass_sim.sort_values(by='s2v_score', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_prep_found_matches_s2v_match_perc_pmass_sim[3].sort_values(by=['s2v_score'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading second test set to test nn output\n",
    "outfile = os.path.join(path_data, 'new_found_matches_s2v_match_perc_pmass_sim.pickle')\n",
    "print(outfile)\n",
    "if os.path.exists(outfile):\n",
    "    with open(outfile, 'rb') as inf:\n",
    "        new_found_matches_s2v_match_perc_pmass_sim = pickle.load(inf)\n",
    "\n",
    "#preprocess for neural network, remove mass_match, make number of matches and parent_mass relative and let s2v_score start at 0\n",
    "nn_prep_new_found_matches_s2v_match_perc_pmass_sim = []\n",
    "for ID in range(len(new_found_matches_s2v_match_perc_pmass_sim)):\n",
    "    current = new_found_matches_s2v_match_perc_pmass_sim[ID].copy()\n",
    "    current = transform_num_matches(current)\n",
    "    current['parent_mass'] = [cur_pm/max_parent_mass for cur_pm in current['parent_mass']]\n",
    "    current.drop(['mass_match', 'perc_mass_change'], axis=1, inplace=True)\n",
    "    q_mass = new_documents_query_s2v[ID]._obj.get(\"parent_mass\")\n",
    "    current = find_mass_difference(current, new_documents_library_s2v, q_mass, base_num = 0.8)\n",
    "    nn_prep_new_found_matches_s2v_match_perc_pmass_sim.append(current)\n",
    "\n",
    "#finalising data for nn, add label 1 if similarity is above cutoff\n",
    "sim_cutoff = 0.6\n",
    "nn_new_found_matches_s2v_match_perc_pmass_sim = nn_prep_new_found_matches_s2v_match_perc_pmass_sim[0]\\\n",
    "    .append(nn_prep_new_found_matches_s2v_match_perc_pmass_sim[1:])\\\n",
    "    .drop(['label'], axis = 1)\n",
    "simlabs = [1 if sim >= sim_cutoff else 0 for sim in nn_new_found_matches_s2v_match_perc_pmass_sim['similarity']]\n",
    "nn_new_found_matches_s2v_match_perc_pmass_sim['sim_label'] = simlabs\n",
    "nn_new_found_matches_s2v_match_perc_pmass_sim.drop(['similarity'], axis = 1, inplace = True)\n",
    "nn_new_found_matches_s2v_match_perc_pmass_sim.sort_values(by='s2v_score', ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN for learning similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "# split into input (X) and output (y) variables\n",
    "X = nn_found_matches_s2v_match_perc_pmass_sim.drop('sim_label', axis = 1)\n",
    "y = nn_found_matches_s2v_match_perc_pmass_sim['sim_label']\n",
    "# define the keras model\n",
    "model = Sequential()\n",
    "model.add(Dense(12, input_dim=X.shape[1], activation='relu'))\n",
    "model.add(Dense(12, activation='relu'))\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dense(12, activation='relu'))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "# compile the keras model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# fit the keras model on the dataset\n",
    "model.fit(X, y, epochs=30, batch_size = 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = nn_new_found_matches_s2v_match_perc_pmass_sim.drop(['sim_label'], axis = 1)\n",
    "y_test = nn_new_found_matches_s2v_match_perc_pmass_sim['sim_label']\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print('Accuracy: %.2f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make class predictions with the model\n",
    "predictions_test = model.predict_classes(X_test)\n",
    "# # summarize the first 5 cases\n",
    "# for i in range(5):\n",
    "#     print('%s => %d (expected %d)' % (X_test.iloc[i].tolist(), predictions_test[i], y_test.iloc[i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_new_testset_tested[0].sort_values(by=['s2v_score'], ascending=False).iloc[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_new_testset_tested = []\n",
    "for ID in range(len(nn_prep_new_found_matches_s2v_match_perc_pmass_sim)):\n",
    "    current_match = nn_prep_new_found_matches_s2v_match_perc_pmass_sim[ID].copy()\n",
    "    simlabs = [1 if sim >= sim_cutoff else 0 for sim in current_match['similarity']]\n",
    "    predict_on = current_match.drop(['label', 'similarity'], axis = 1)\n",
    "    predictions = model.predict_classes(predict_on)\n",
    "    current_match['predictions'] = predictions\n",
    "    correct_guess = [pred == lab for pred, lab in zip(predictions, simlabs)]\n",
    "    current_match['correct_guess'] = correct_guess\n",
    "    nn_new_testset_tested.append(current_match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if top hit is not true match, what is the average similarity of top hit above a threshold?\n",
    "topn = 20\n",
    "threshold = 0.4\n",
    "new_false_m_simils_thresh = []\n",
    "new_ids_top_false_hits_thresh = []\n",
    "for ID in range(len(nn_new_testset_tested)):\n",
    "    df_select = nn_new_testset_tested[ID].sort_values(by=['s2v_score'], ascending = False).iloc[:topn]\n",
    "    df_select = df_select[(df_select['s2v_score'] > threshold)]\n",
    "    if df_select.shape[0] > 0:\n",
    "        top_hit = df_select.iloc[0]\n",
    "        if top_hit['label'] == 0:\n",
    "            new_false_m_simil = top_hit['similarity']\n",
    "            print(ID, new_false_m_simil)\n",
    "            new_ids_top_false_hits_thresh.append(ID)\n",
    "            new_false_m_simils_thresh.append(new_false_m_simil)\n",
    "print(len(new_false_m_simils_thresh))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(2,2))\n",
    "plt.boxplot(new_false_m_simils_thresh)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing if top hit is above 0.6 similarity after filtering with nn\n",
    "#look at top20\n",
    "test_sim_mcos_top20 = []\n",
    "test_sim_s2v_top20 = []\n",
    "test_sim_filt_nn = []\n",
    "ideal_case_top20 = []\n",
    "\n",
    "perc_m_change_thresh = 0\n",
    "threshold = 0.4\n",
    "topn = 20 #only look at top 20\n",
    "sim_threshold = 0.6\n",
    "\n",
    "for ID in range(len(nn_new_testset_tested)):\n",
    "    # mass + mod cosine\n",
    "    current_match = nn_new_testset_tested[ID].sort_values(by=['mod_cosine_score'], ascending=False).iloc[:topn]\n",
    "\n",
    "    df_select = current_match[(current_match['perc_mass_change'] <= perc_m_change_thresh)\n",
    "                                  & (current_match['mod_cosine_score'] > threshold)]\n",
    "    if df_select.shape[0] > 0:\n",
    "        #best_match_ID = df_select.sort_values(by=['mod_cosine_score'], ascending=False).index[0]\n",
    "        #inchikey_match = documents_library_s2v[best_match_ID]._obj.get(\"inchikey\")[:14]\n",
    "        sim = df_select.iloc[0]['similarity'] #look at similarity of top hit\n",
    "        best_bet = 1 * (sim >= sim_threshold)\n",
    "    else:\n",
    "        best_bet = -1 # meaning: not found\n",
    "    test_sim_mcos_top20.append(best_bet)\n",
    "\n",
    "    # Scenario 1: mass + Spec2Vec + threshold\n",
    "    current_match = nn_new_testset_tested[ID].sort_values(by=['s2v_score'], ascending = False).iloc[:topn]\n",
    "    \n",
    "    #inchikey_query = documents_query_s2v[ID]._obj.get(\"inchikey\")[:14]\n",
    "    #select top20\n",
    "    #current_match = found_matches_s2v[ID].sort_values(by=['s2v_score'], ascending=False).iloc[:topn]\n",
    "\n",
    "    df_select = current_match[(current_match['perc_mass_change'] <= perc_m_change_thresh)\n",
    "                                  & (current_match['s2v_score'] > threshold)]\n",
    "    if df_select.shape[0] > 0:\n",
    "        #best_match_ID = df_select.sort_values(by=['s2v_score'], ascending=False).index[0]\n",
    "        #inchikey_match = documents_library_s2v[best_match_ID]._obj.get(\"inchikey\")[:14]\n",
    "        sim = df_select.iloc[0]['similarity'] #look at similarity of top hit\n",
    "        best_bet = 1 * (sim >= sim_threshold)\n",
    "    else:\n",
    "        best_bet = -1 # meaning: not found\n",
    "    test_sim_s2v_top20.append(best_bet)\n",
    "    \n",
    "    # Scenario 2: Spec2Vec + threshold after filtering for non-similar matches with nn\n",
    "    df_select = current_match[(current_match['perc_mass_change'] <= perc_m_change_thresh)\n",
    "                                  & (current_match['predictions'] == 1)\n",
    "                                  & (current_match['s2v_score'] > threshold)]\n",
    "    if df_select.shape[0] > 0:\n",
    "        #best_match_ID = df_select.sort_values(by=['s2v_score'], ascending=False).index[0]\n",
    "        #inchikey_match = documents_library_s2v[best_match_ID]._obj.get(\"inchikey\")[:14]\n",
    "        sim = df_select.iloc[0]['similarity'] #look at similarity of top hit\n",
    "        best_bet = 1 * (sim >= sim_threshold)\n",
    "    else:\n",
    "        best_bet = -1 # meaning: not found\n",
    "    test_sim_filt_nn.append(best_bet)\n",
    "    \n",
    "    # ideal case: how many queries have an actual match above s2v threshold and >0.6 tanimoto\n",
    "    df_select = current_match[(current_match['s2v_score'] > threshold)]\n",
    "    best_bet = -1\n",
    "    if df_select.shape[0] > 0:\n",
    "        if any(df_select['similarity'] > 0.6):\n",
    "            best_bet = 1\n",
    "    ideal_case_top20.append(best_bet)\n",
    "\n",
    "\n",
    "#make arrays from lists\n",
    "test_sim_mcos_arr_top20 = np.array(test_sim_mcos_top20)\n",
    "test_sim_s2v_arr_top20 = np.array(test_sim_s2v_top20)\n",
    "test_sim_filt_nn_arr = np.array(test_sim_filt_nn)\n",
    "ideal_case_arr_top20 = np.array(ideal_case_top20) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### plt.style.use('ggplot')\n",
    "\n",
    "ylabels = ['mass + mod cosine',\n",
    "           'mass + Spec2Vec',\n",
    "           'mass + Spec2Vec + filtering NN',\n",
    "           'ideal case Spec2Vec>0.4']\n",
    "xlabels = ['true matches', 'false matches', 'no matches found']\n",
    "          #'mass + mod.cosine \\n + as backup: Spec2Vec + mod.cosine']\n",
    "data = np.array([[np.sum(test_sim_mcos_arr_top20 == 1), np.sum(test_sim_mcos_arr_top20 == 0), np.sum(test_sim_mcos_arr_top20 == -1)],\n",
    "                 [np.sum(test_sim_s2v_arr_top20 == 1), np.sum(test_sim_s2v_arr_top20 == 0), np.sum(test_sim_s2v_arr_top20 == -1)],\n",
    "                 [np.sum(test_sim_filt_nn_arr == 1), np.sum(test_sim_filt_nn_arr == 0), np.sum(test_sim_filt_nn_arr == -1)],\n",
    "                 [np.sum(ideal_case_arr_top20 == 1), np.sum(ideal_case_arr_top20 == 0), np.sum(ideal_case_arr_top20 == -1)]])\n",
    "\n",
    "dataframe = pd.DataFrame(data, columns=xlabels, index=ylabels)\n",
    "\n",
    "#fig, ax = plt.subplots(figsize=(5, 5))\n",
    "ax = dataframe.sort_values(by=['false matches'], ascending=True).plot.barh(stacked=True, edgecolor='none', figsize=(5, 3))\n",
    "\n",
    "horiz_offset = 1.03\n",
    "vert_offset = 1.\n",
    "ax.legend(bbox_to_anchor=(horiz_offset, vert_offset))\n",
    "plt.xlabel('Number of spectra')\n",
    "plt.title('Compare if top hit is above 0.6 tanimoto similarity\\n(>5 matching inchikey in library)')\n",
    "#plt.figure(figsize=(5,5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN for learning true hits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing if top hit is a true hit after filtering for similarity with nn\n",
    "#look at top20\n",
    "test_m_new_s2v_top20 = []\n",
    "test_m_new_filt_nn_top20 = []\n",
    "\n",
    "threshold = 0.4\n",
    "topn = 20 #only look at top 20\n",
    "sim_threshold = 0.6\n",
    "\n",
    "for ID in range(len(nn_new_testset_tested)):\n",
    "    current_match = nn_new_testset_tested[ID].sort_values(by=['s2v_score'], ascending = False).iloc[:topn]\n",
    "    \n",
    "    inchikey_query = documents_query_s2v[ID]._obj.get(\"inchikey\")[:14]\n",
    "    #select top20\n",
    "    #current_match = found_matches_s2v[ID].sort_values(by=['s2v_score'], ascending=False).iloc[:topn]\n",
    "\n",
    "    # Scenario 1: mass + Spec2Vec + threshold\n",
    "    df_select = current_match[(current_match['perc_mass_change'] == 0)\n",
    "                                  & (current_match['s2v_score'] > threshold)]\n",
    "    if df_select.shape[0] > 0:\n",
    "        #best_match_ID = df_select.sort_values(by=['s2v_score'], ascending=False).index[0]\n",
    "        #inchikey_match = documents_library_s2v[best_match_ID]._obj.get(\"inchikey\")[:14]\n",
    "        sim = df_select.iloc[0]['similarity'] #look at similarity of top hit\n",
    "        best_bet = 1 * (sim >= sim_threshold)\n",
    "    else:\n",
    "        best_bet = -1 # meaning: not found\n",
    "    test_m_new_s2v_top20.append(best_bet)\n",
    "    \n",
    "    # Scenario 2: Spec2Vec + threshold after filtering for non-similar matches with nn\n",
    "    df_select = current_match[(current_match['perc_mass_change'] == 0)\n",
    "                                  & (current_match['predictions'] == 1)\n",
    "                                  & (current_match['s2v_score'] > threshold)]\n",
    "    if df_select.shape[0] > 0:\n",
    "        #best_match_ID = df_select.sort_values(by=['s2v_score'], ascending=False).index[0]\n",
    "        #inchikey_match = documents_library_s2v[best_match_ID]._obj.get(\"inchikey\")[:14]\n",
    "        sim = df_select.iloc[0]['similarity'] #look at similarity of top hit\n",
    "        best_bet = 1 * (sim >= sim_threshold)\n",
    "    else:\n",
    "        best_bet = -1 # meaning: not found\n",
    "    test_m_new_filt_nn_top20.append(best_bet)\n",
    "\n",
    "\n",
    "#make arrays from lists\n",
    "test_m_new_s2v_arr_top20 = np.array(test_m_new_s2v_top20)\n",
    "test_m_new_filt_nn_arr_top20 = np.array(test_m_new_filt_nn_top20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
